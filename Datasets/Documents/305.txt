Generative Pre-trained Transformer 3 (Türkçe: Üretken Ön İşlemeli Dönüştürücü 3) kısaca GPT-3, insanların yazdığı metinlere benzer içerik üretmek için derin öğrenmeyi kullanan özbağlanımlı dil modelidir. GPT-n serisindeki üçüncü nesil dil tahmin modeli olan GPT-3, San Francisco merkezli yapay zeka araştırma laboratuvarı OpenAI tarafından geliştirilmiştir. GPT-3'ün tam sürümü, veri işleyecek 175 milyar parametreye sahiptir. Bu rakam GPT-2'nin öğrenme kapasitesinin 2 katıdır. 14 Mayıs 2020'de tanıtılan ve Temmuz 2020 itibarıyla beta aşamasında olan GPT-3, önceden öğretilmiş dil örnekleriyle doğal dil işleme (NLP) sistemini kullanmaktadır. GPT-3'ün piyasaya sürülmesinden önce, en büyük dil modeli Microsoft'un Şubat 2020'de tanıttığı ve GPT-3'ün %10'undan daha az kapasiteye sahip olan (17 milyar parametre) Turing NLG idi. GPT-3 tarafından üretilen metnin kalitesi o kadar yüksektir ki, bir insan tarafından yazılmadığını anlamak zordur. Bu durumun yaratabileceği avantajlar olabileceği gibi riskler de vardır. 31 OpenAI araştırmacısı ve mühendisi, 28 Mayıs 2020'de GPT-3'ü tanıtan orijinal makaleyi yayımlamıştır. Makalelerinde, GPT-3'ün potansiyel tehlikeleri konusunda insanları uyarmış ve bu riski azaltmak için çalışmalar yapılması gerektiği çağrısında bulunmuşlardır. Avustralyalı filozof David Chalmers, GPT-3'ü 'şimdiye kadar üretilmiş en ilginç ve önemli yapay zeka sistemlerinden biri' olarak tanımlamıştır. The Economist'e göre, geliştirilmiş algoritmalar, güçlü bilgisayarlar ve sayısallaştırılmış verilerdeki artış, makine öğreniminde bir devrimi tetiklemiştir. Yazılım modelleri, bir 'yapıda' binlerce veya milyonlarca örnek kullanarak öğrenmek üzere eğitilmektedir. Doğal dil işlemede (NLP) kullanılan mimari, ilk olarak 2017'de tanıtılan derin öğrenme modeline dayanan Transformer adlı yapay sinir ağıdır. GPT-n modellerinin yapısı, Transformer tabanlı derin öğrenme sistemine dayanmaktadır. Veri işleme, veri madenciliği, veri düzenleme, veriler arası bağlantı kurma, verilerde zıtlık oluşturma, verileri anlama ve sorulara yanıt verme becerisine sahip bir dizi NLP sistemi vardır. OpenAI'de çalışan 31 mühendis ve araştırmacıdan oluşan grup projeyi 28 Mayıs 2020'de tanıttı. Ekip, GPT-3'ün kapasitesinin selefi GPT-2'ye nazaran iki kat fazla olduğunu ve türünün en gelişmiş örneği olduğunu belirtmiştir. GPT-3'ün parametre sayısı, daha küçük kapasiteli önceki sürümlere kıyasla daha doğru çalışmasını sağlamaktadır. GPT-3'ün kapasitesi, Microsoft'un Turing NLG'sinin on katından fazladır. GPT-3'e öğrenmesi için verilen veri kümesinin %60'ının kaynağı, 410 milyar veriden oluşan filtrelenmiş Common Crawl sürümüdür. Veri kümesinin %22'si İnternetteki verilerden, %16'sı şimdiye kadar yayımlanmış kitaplardan ve %3'ü Wikipedia'dan gelmektedir. GPT-3, yüz milyarlarca kelimenin yanı sıra CSS, JSX, Python'da kodlama yeteneğine sahiptir. GPT-3'ün eğitim verileri her şeyi kapsadığından, farklı dil görevleri için daha fazla veri öğrenmesi gerekmemektedir. 11 Haziran 2020'de OpenAI, kullanıcıların OpenAI'nin yeni teknolojisinin 'güçlü yönlerini ve sınırlarını keşfetmesine' yardımcı olmak için kullanıcı dostu GPT-3 API'sine ('makine öğrenimi araç seti') erişim talep edebileceklerini duyurmuştur. Davetiyede, bu API'nin neredeyse 'tüm İngilizce komutları' tamamlayabildiğini yazmaktadır. OpenAI GPT-3 API'nin erken sürümüne erişim hakkı olan bir kullanıcıya göre, GPT-3 'inanılmaz derecede tutarlı metinler' yazma konusunda 'ürkütücü derecede iyiydi'. GPT-3, insanlar tarafından yazılan makalelerden ayırt etmekte güçlük çekilen haberler üretebildiğinden', GPT-3'ün hem yararlı hem de zararlı uygulamaların geliştirilmesinde kullanılabileceği düşünülmektedir. Araştırmacılar 28 Mayıs 2020 tarihli makalelerinde GPT-3'ün potansiyel zararlı etkilerini detaylıca açıklamışlardır. Yanlış bilgi üretimi, spam, kimlik avı, yasal ve hükûmet süreçlerinin kötüye kullanımı, sahte akademik makale yazma ve sosyal mühendislik potansiyel zararlı etkilerden bazılarıdır. Yazarlar, bu risklerin azaltılması konusunda araştırmalar yapılması için bu tehlikelere dikkat çekmektedir.